b'<!DOCTYPE html><br><br><html data-wf-page="5bbf5e00b5282f27efacb215" data-wf-site="5bbf5e00b5282f228dacb214"><br><head><br><meta charset="utf-8"/><br><title>WHAT Lab</title><br><meta content="Discover HKUST, an international research university ranked #1 among Asia Young Universities, focused on experiential teaching, research and learning, dedicated to nurturing global leaders and innovative entrepreneurial professionals." name="description"/><br><meta content="WHAT Lab" property="og:title"/><br><meta content="Discover HKUST, an international research university ranked #1 among Asia Young Universities, focused on experiential teaching, research and learning, dedicated to nurturing global leaders and innovative entrepreneurial professionals." property="og:description"/><br><meta content="summary" name="twitter:card"/><br><meta content="width=device-width, initial-scale=1" name="viewport"/><br><link href="css/normalize.css" rel="stylesheet" type="text/css"/><br><link href="css/components.css" rel="stylesheet" type="text/css"/><br><link href="css/whatlab.css" rel="stylesheet" type="text/css"/><br><script src="https://ajax.googleapis.com/ajax/libs/webfont/1.4.7/webfont.js" type="text/javascript"></script><br><script type="text/javascript"><br>      WebFont.load({<br>        google: {<br>          families: ["Muli:300,regular,600,700,800:latin,latin-ext"]<br>        }<br>      });<br>    </script><br><!-- [if lt IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" type="text/javascript"></script><![endif] --><br><script type="text/javascript"><br>      ! function(o, c) {<br>        var n = c.documentElement,<br>          t = " w-mod-";<br>        n.className += t + "js", ("ontouchstart" in o || o.DocumentTouch && c instanceof DocumentTouch) && (n.className += t + "touch")<br>      }(window, document);<br>    </script><br><link href="images/HKUST-fav-32_1HKUST-fav-32.png" rel="shortcut icon" type="image/x-icon"/><br><link href="images/HKUST-fav-256.png" rel="apple-touch-icon"/><br><script type="text/javascript"><br>      (function(i, s, o, g, r, a, m) {<br>       i[\'GoogleAnalyticsObject\'] = r;<br>       i[r] = i[r] || function() {<br>       (i[r].q = i[r].q || []).push(arguments)<br>       }, i[r].l = 1 * new Date();<br>       a = s.createElement(o), m = s.getElementsByTagName(o)[0];<br>       a.async = 1;<br>       a.src = g;<br>       m.parentNode.insertBefore(a, m)<br>       })(window, document, \'script\', \'https://www.google-analytics.com/analytics.js\', \'ga\');<br>       ga(\'create\', \'UA-127031945-1\', \'auto\');<br>       ga(\'send\', \'pageview\');<br>  </script><br><style media="screen" type="text/css"><br>      @media (max-width: 991px) {<br>        .nav-menu {<br>          overflow-y: auto;<br>          overflow-x: hidden;<br>        }<br>      }<br>      .nav-menu {<br>        -webkit-touch-callout: none;<br>        -webkit-user-select: none;<br>        -khtml-user-select: none;<br>        -moz-user-select: none;<br>        -ms-user-select: none;<br>        -o-user-select: none;<br>        user-select: none;<br>      }<br>    </style><br><!--  Bootstrap Core CSS  --><br><link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet"/><br><!--  Theme CSS  --><br><link href="css/freelancer.css" rel="stylesheet"/><br><!--  Custom CSS  --><br><link href="css/main.css" rel="stylesheet"/><br><!--  Custom Fonts  --><br><link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><br><link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css"/><br><link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css"/><br><!--  HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries  --><br><!--  WARNING: Respond.js doesn\'t work if you view the page via file://  --><br><!-- [if lt IE 9]><br><script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script><br><script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script><br><![endif] --><br><style><br>    .arr {<br>    margin-left: 5px;<br>    }<br>    @media (max-width: 991px) {<br>        .arr {<br>        display: block;<br>        }<br>    }<br>    @media (max-width: 479px) {<br>        .more-links {<br>        letter-spacing: 0px;<br>        }<br>        .footer-copyright {<br>        font-size: 11px;<br>        letter-spacing: 0px;<br>        }<br>    }<br>    </style><br></head><br><body class="body-muli"><br><div class="header" data-w-id="c6788095-1fea-2ff1-3052-d6e2edd183be" style="opacity:1;display:block"><br><div class="reveal-section"><br><div class="reveal-div" data-w-id="7d5afc81-a25e-01e8-be7b-5f7af61e4fad" style="height:0PX"><br><div class="container w-container"><br><div class="reveal-content"><br><div class="reveal-title">MOREABOUTHKUST</div><br><div class="more-links-wrap"><br><div class="more-links-flex"><a class="more-links" href="https://www.ust.hk/news" target="_blank">University News</a></div><br><div class="more-links-flex"><a class="more-links" href="https://www.ust.hk/academics/list" target="_blank">Academic Departments A-Z</a></div><br><div class="more-links-flex"><a class="more-links" href="https://www.ust.hk/lifehkust" target="_blank">Life@HKUST</a></div><br><div class="more-links-flex"><a class="more-links" href="http://library.ust.hk/" target="_blank">Library</a></div><br><div class="more-links-flex"><a class="more-links" href="https://www.ust.hk/map-directions" target="_blank">Map &amp; Directions</a></div><br><div class="more-links-flex"><a class="more-links" href="https://www.ab.ust.hk/hro/PubDoc/careers/main.html?btn1=btn_nav_06&amp;main=welcome.html" target="_blank">Jobs@HKUST</a></div><br><div class="more-links-flex"><a class="more-links" href="https://facultyprofiles.ust.hk/" target="_blank">Faculty Profiles</a></div><br><div class="more-links-flex"><a class="more-links" href="https://www.ust.hk/about-hkust" target="_blank">About HKUST</a></div><br></div><br></div><br></div><br></div><br></div><br><div class="top-bar"><br><div class="max1140-w"><br><div class="top-bar-inner-div w-clearfix"><br><div class="search-overlay-div"><br><h1 class="search-title-o">Search</h1><br><form action="/search" class="search-block-o w-clearfix w-form"><input autofocus="true" class="search-input-o w-input" id="search-o" maxlength="256" name="query" placeholder="Type here to search..." required="" type="search"/><input class="search-button-o w-button" type="submit" value="Search"/></form><br><div class="search-overlay-close"></div><br></div><br><div class="reveal-top-btn" data-w-id="d734f6ab-3049-cde0-8f2a-98e4f87bef76"><img alt="More About HKUST" class="reveal-top-arrow" data-w-id="83046d68-0697-a001-de28-a36fa3317bfa" height="8" src="images/chevron-up.svg" style="-webkit-transform:translate3d(0, 0, 0) scale3d(1, 1, 1) rotateX(0) rotateY(0) rotateZ(0DEG) skew(0, 0);-moz-transform:translate3d(0, 0, 0) scale3d(1, 1, 1) rotateX(0) rotateY(0) rotateZ(0DEG) skew(0, 0);-ms-transform:translate3d(0, 0, 0) scale3d(1, 1, 1) rotateX(0) rotateY(0) rotateZ(0DEG) skew(0, 0);transform:translate3d(0, 0, 0) scale3d(1, 1, 1) rotateX(0) rotateY(0) rotateZ(0DEG) skew(0, 0)"/></div><a class="top-bar-link" href="https://www.cse.ust.hk/BDI/" target="_blank">Big Data Institute</a></div><br></div><br></div><br><div class="nav-section"><br><div class="main-menu-wrap-div"><br><div class="main-menu-div"><br><div class="max1140-w"><br><div class="menu-logo-row"><br><div class="scroll-spacer-a w-hidden-medium w-hidden-small w-hidden-tiny"></div><br><div class="logo-search-flex"><br><div class="logo-row-inline"><br><div class="logo-row-flex"><a class="hkust-logo-link w-inline-block" href="https://www.ust.hk" target="_blank"><img alt="HKUST" class="hkust-logo" height="56" src="images/HKUST-svg-logo.svg"/></a><a class="site-logo-link w-inline-block w--current" href="index.html"><img alt="WHAT Lab" class="site-logo" height="20" src="images/WHAT-LAB.svg"/></a></div><br></div><br></div><br><div class="scroll-spacer-b w-hidden-medium w-hidden-small w-hidden-tiny"></div><br></div><br></div><br></div><br></div><br></div><br></div><br><div class="header-reveal-spacer" data-w-id="5c3a0f88-65db-f415-8490-432f9e0e3bcc" style="height:0PX"></div><br><div class="content-section"><br><div class="content-offset-spacer"></div><br><div class="fade-in-trigger" data-w-id="4fad33f6-b4a3-d16d-f9e3-4ae17c743fab"></div><br><div id="skipnav"><a href="#myCarousel">Skip to main content</a></div><br><!-- Header --><br><header class="carousel slide" id="myCarousel"><br><!-- Indicators --><br><ol class="carousel-indicators"><br><li class="active" data-slide-to="0" data-target="#myCarousel"></li><br><li data-slide-to="1" data-target="#myCarousel"></li><br><li data-slide-to="2" data-target="#myCarousel"></li><br><li data-slide-to="3" data-target="#myCarousel"></li><br><li data-slide-to="4" data-target="#myCarousel"></li><br></ol><br><!-- Wrapper for slides --><br><div class="carousel-inner"><br><div class="item page-scroll active"><br><a class="portfolio-link" data-toggle="modal" href="#portfolioModal1"><div class="fill" style="background-image:url(\'img/banners/Slide1.jpg\');"></div></a><br></div><br><div class="item page-scroll"><br><a class="portfolio-link" data-toggle="modal" href="#portfolioModal2"><div class="fill" style="background-image:url(\'img/banners/Slide2.jpg\');"></div></a><br></div><br><div class="item page-scroll"><br><a class="portfolio-link" data-toggle="modal" href="#portfolioModal3"><div class="fill" style="background-image:url(\'img/banners/Slide3.jpg\');"></div></a><br></div><br><div class="item page-scroll"><br><a class="portfolio-link" data-toggle="modal" href="#portfolioModal4"><div class="fill" style="background-image:url(\'img/banners/Slide4.jpg\');"></div></a><br></div><br><div class="item page-scroll"><br><a class="portfolio-link" data-toggle="modal" href="#portfolioModal5"><div class="fill" style="background-image:url(\'img/banners/Slide5.jpg\');"></div></a><br></div><br></div><br><!-- Controls --><br><a class="left carousel-control" data-slide="prev" href="#myCarousel"><br><span class="icon-prev"></span><br></a><br><a class="right carousel-control" data-slide="next" href="#myCarousel"><br><span class="icon-next"></span><br></a><br></header><br><!-- Navigation --><br><nav class="navbar navbar-default navbar-custom" data-offset-top="405" data-spy="affix" id="mainNav"><br><div class="container"><br><!-- Brand and toggle get grouped for better mobile display --><br><div class="navbar-header page-scroll"><br><button class="navbar-toggle" data-target="#bs-example-navbar-collapse-1" data-toggle="collapse" type="button"><br><span class="sr-only">Toggle navigation</span> Menu <i class="fa fa-bars"></i><br></button><br><a class="navbar-brand" href="#page-top">WHAT Lab</a><br></div><br><!-- Collect the nav links, forms, and other content for toggling --><br><div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1"><br><ul class="nav navbar-nav navbar-right" role="tablist"><br><li class="hidden"><br><a href="#page-top"></a><br></li><br><li class="page-scroll"><br><a href="#Intro">Introduction</a><br></li><br><li class="page-scroll"><br><a href="#portfolio">Research areas</a><br></li><br><!--<br>                     <li class="dropdown page-scroll"><br>                     <a href="#NLP" class="dropdown-toggle" data-toggle="dropdown" role="button"<br>                     aria-haspopup="true" aria-expanded="false">Research areas<span class="caret"></span></a><br>                     <ul class="dropdown-menu page-scroll"><br>                     <li><a class="dropdown-item" href="#NLP">Natural Language Processing</a></li><br>                     <li><a class="dropdown-item" href="#IPAV">Information Propagation and Visualization</a></li><br>                     <li><a class="dropdown-item" href="#LML">Large-scale Machine Learning</a></li><br>                     <li><a class="dropdown-item" href="#VA">Video Analysis</a></li><br>                     <li><a class="dropdown-item" href="#ROB">Robotics</a></li><br>                     </ul><br>                     </li><br>                     --><br><li class="page-scroll"><br><a href="#about">News and Events</a><br></li><br><li class="page-scroll"><br><a href="#contact">Demos</a><br></li><br></ul><br></div><br><!-- /.navbar-collapse --><br></div><br><!-- /.container-fluid --><br></nav><br><!-- Introduction Section --><br><section class="intro-section" id="Intro"><br><div class="intro-section intro-image"><br><div class="intro-space"></div><br><div class="intro-text"><br>                WHAT LAB, which is short for WeChat-HKUST Joint Lab on Artificial Intelligence<br>                Technology, is dedicated to foster artificial intelligence and big data research<br>                to improve peoples living and advance the frontiers of knowledge, marking a milestone<br>                in the collaboration of WeChat and the higher education sector.<br>                <br/><br><br/><br>                WeChat and HKUST will jointly conduct Artificial Intelligence (AI) Technology related<br>                research and explore the far-reaching frontiers of AI. Today AI technology is experiencing<br>                a tremendous growth, and much of this advance depends on talents, problems and data. WeChat<br>                and HKUST complement each other in these aspects, and this collaboration on AI research is<br>                expected to be long-term and world-leading. Research areas of WHAT LAB include intelligent<br>                robotic systems, natural language processing, data mining, speech recognition and<br>                understanding. The Lab will bring together top researchers in the development of innovative<br>                artificial intelligence application with the database of WeChat.<br>            </div><br></div><br></section><br><!-- Portfolio Grid Section --><br><section id="portfolio"><br><div class="container"><br><div class="row"><br><div class="col-lg-12 text-center"><br><h2>Research areas</h2><br><hr class="star-primary"/><br></div><br></div><br><div class="row"><br><div class="col-sm-4 portfolio-item"><br><a class="portfolio-link" data-toggle="modal" href="#portfolioModal1"><br><div class="caption"><br><div class="caption-content"><br><i class="fa fa-search-plus fa-3x"></i><br></div><br></div><br><img alt="" class="img-responsive" src="img/areas/Slide1.jpg"/><br></a><br></div><br><div class="col-sm-4 portfolio-item"><br><a class="portfolio-link" data-toggle="modal" href="#portfolioModal2"><br><div class="caption"><br><div class="caption-content"><br><i class="fa fa-search-plus fa-3x"></i><br></div><br></div><br><img alt="" class="img-responsive" src="img/areas/Slide2.jpg"/><br></a><br></div><br><div class="col-sm-4 portfolio-item"><br><a class="portfolio-link" data-toggle="modal" href="#portfolioModal3"><br><div class="caption"><br><div class="caption-content"><br><i class="fa fa-search-plus fa-3x"></i><br></div><br></div><br><img alt="" class="img-responsive" src="img/areas/Slide3.jpg"/><br></a><br></div><br><div class="col-sm-2 portfolio-item"><br></div><br><div class="col-sm-4 portfolio-item"><br><a class="portfolio-link" data-toggle="modal" href="#portfolioModal4"><br><div class="caption"><br><div class="caption-content"><br><i class="fa fa-search-plus fa-3x"></i><br></div><br></div><br><img alt="" class="img-responsive" src="img/areas/Slide4.jpg"/><br></a><br></div><br><div class="col-sm-4 portfolio-item"><br><a class="portfolio-link" data-toggle="modal" href="#portfolioModal5"><br><div class="caption"><br><div class="caption-content"><br><i class="fa fa-search-plus fa-3x"></i><br></div><br></div><br><img alt="" class="img-responsive" src="img/areas/Slide5.jpg"/><br></a><br></div><br></div><br></div><br></section><br><!-- About Section --><br><section class="success" id="about"><br><div class="container"><br><div class="row"><br><div class="col-lg-12 text-center"><br><h2>News and Events</h2><br><hr class="star-light"/><br></div><br></div><br><div class="row"><br><div class="col-sm-4"><br><a href="https://mp.weixin.qq.com/s?__biz=MzI5MDAwOTIzOQ==&amp;mid=2650901437&amp;idx=1&amp;sn=cfd1d218015408c3b8e16d7122c61c98&amp;chksm=f7d35245c0a4db53a8faac7b16e2019861a0bbd5310355edfd23e01cb53863c756f66e9107e5&amp;mpshare=1&amp;scene=1&amp;srcid=1118dqGJRuK6xIeE4gTkmot9&amp;key=0d81900dc147014eedbc7386b722581a6bddcbaa1f9a89d52f44ba557a6639d72fecd5bf2b8f76bc654922f3128147b56418582279a51fe88dddf6dc0eb7f4206a5d24d9559bce942472c939931e0bbc&amp;ascene=0&amp;uin=ODk4MTAyMDAw&amp;devicetype=iMac+MacBookPro12%2C1+OSX+OSX+10.11+build(15A284)&amp;version=11020201&amp;pass_ticket=FwVDrOFvufzqym4TyO%2FhO3UA0jwINc2ZRt0TB2oZri1Q%2BGvtcWH3xY7N81nDqv9r"><br><div class="thumbnail"><br><img alt="" src="img/news/1.jpg"/><br><div class="caption"><br><h3>One-year Anniversary of WHAT Lab in Guangzhou    </h3><br><p class="text-muted"><i aria-hidden="true" class="fa fa-calendar"></i>  Nov 16 2016</p><br></div><br></div><br></a><br></div><br><div class="col-sm-4"><br><a href="https://mp.weixin.qq.com/s?__biz=MzI5MjMyMjA0Mg==&amp;mid=2247484011&amp;idx=1&amp;sn=31220032335b6fb079b785999fc5c535&amp;chksm=ec026dfedb75e4e82fa1c0c9a3bdeb8260c993f785c243e154f83c99eebf908f98e7ddd6564a&amp;mpshare=1&amp;scene=1&amp;srcid=1123Y7zdEZCa9V3tucmOcQ8a&amp;key=0d81900dc147014ec66136bc5fcc3cb52573f64e27bd4eb7809ecc5bda863828de6b7a5911bda87406a44e157ec1804feea023d155e3200a515a5b31367ff4a0380bcbc717c8395041c7c69232fac3a8&amp;ascene=0&amp;uin=ODk4MTAyMDAw&amp;devicetype=iMac+MacBookPro12%2C1+OSX+OSX+10.11+build(15A284)&amp;version=11020201&amp;pass_ticket=FwVDrOFvufzqym4TyO%2FhO3UA0jwINc2ZRt0TB2oZri1Q%2BGvtcWH3xY7N81nDqv9r"><br><div class="thumbnail"><br><img alt="" src="img/news/2.jpg"/><br><div class="caption"><br><h3>Achievement sharing in WHAT Labs one-year anniversary</h3><br><p class="text-muted"><i aria-hidden="true" class="fa fa-calendar"></i>  Nov 16 2016</p><br></div><br></div><br></a><br></div><br><div class="col-sm-4"><br><a href="https://mp.weixin.qq.com/s?__biz=MzI5MDAwOTIzOQ==&amp;mid=2650901442&amp;idx=1&amp;sn=ae2b21bb97e08ca9ad09d1825d3557c3&amp;chksm=f7d3523ac0a4db2c39ead66e6f67d5cd8e8a353e73ef5e8f9d69b8c14d076a292b4f521fc1ca&amp;mpshare=1&amp;scene=1&amp;srcid=1230BkNtU3VXyezsIS5QeCRp&amp;key=da0ef6e2382fd5a36afa7ea69475a0f507de288c880d7aa3c1d994de802ee348c98bb1a15f9ce34801cb252e2b34466593f0631cdc0107bcca6de62e0e6c09f89df05a8cebb25deaf2e06501829e32eb&amp;ascene=0&amp;uin=ODk4MTAyMDAw&amp;devicetype=iMac+MacBookPro12%2C1+OSX+OSX+10.11+build(15A284)&amp;version=11020201&amp;pass_ticket=FwVDrOFvufzqym4TyO%2FhO3UA0jwINc2ZRt0TB2oZri1Q%2BGvtcWH3xY7N81nDqv9r"><br><div class="thumbnail"><br><img alt="" src="img/news/3.jpg"/><br><div class="caption"><br><h3>Data Visualization: Gods Perspective in WeChat       </h3><br><p class="text-muted"><i aria-hidden="true" class="fa fa-calendar"></i>  Dec 30 2016</p><br></div><br></div><br></a><br></div><br><div class="col-sm-4"><br></div><br><div class="col-sm-4"><br><a href="http://www.ust.hk/zh-hans/abouthkust-zh-hans/media-relations-zh-hans/press-releases-zh-hans/hkust-wechat-establish-joint-lab-artificial-intelligence-technology/"><br><div class="thumbnail"><br><img alt="" src="img/news/4.jpg"/><br><div class="caption"><br><h3>WHAT Lab Establishment in HKUST</h3><br><p class="text-muted"><i aria-hidden="true" class="fa fa-calendar"></i>  Nov 26 2015</p><br></div><br></div><br></a><br></div><br><div class="col-sm-4"><br></div><br></div><br></div><br></section><br><!-- Contact Section --><br><section id="contact"><br><div class="container"><br><div class="row"><br><div class="col-lg-12 text-center"><br><h2>Demos</h2><br><hr class="star-primary"/><br></div><br></div><br><div class="row"><br><div class="col-lg-10 col-lg-offset-1"><br><div class="demo-item"><br><h3 class="text-center">Machine Reading system</h3><br><h4 class="text-muted text-center">Developed by Yuxiang Wu, Prof. Qiang Yangs Group</h4><br><video controls="controls" src="demo/1.mp4">Your browser does not support the video tag.</video><br><p class="demo-text">Demo Introduction: Machine Reading aims to develop Machine Learning algorithms<br>                        that could read and comprehend natural language documents as humans do. With Machine<br>                        Reading, natural language information is converted to the form that could be processed<br>                        by computers, and could be further utilized in applications such as summarization,<br>                        question answering and dialogue system.</p><br></div><br><div class="demo-item"><br><h3 class="text-center">Moments Articles Real Time Propagation and Visualization System</h3><br><h4 class="text-muted text-center">Developed by Quan Li, Dongyu Liu, Haiyan Yang, Prof. Huamin Qus Group</h4><br><video controls="controls" src="demo/2.mp4">Your browser does not support the video tag.</video><br><p class="demo-text">Demo Introduction: In this project, we visually investigated how official<br>                        public account article information propagate in WeChat platform from different perspectives,<br>                        involving a 3D global overview, time-varying propagation view, community detection view, etc.<br>                        We also implemented several designs by using real propagation data, including the propagation<br>                        clock, propagation wave, propagation galaxy and propagating tree. The system --- WeSeer has<br>                        already been deployed and applied to WeChat, Tencent for daily propagation analysis.</p><br></div><br><div class="demo-item"><br><h3 class="text-center">Model-based Global Localization for Aerial Robots using Edge Alignments</h3><br><h4 class="text-muted text-center">Developed by Kejie Qiu, Prof. Shaojie Shens Group</h4><br><video controls="controls" src="demo/3.mp4">Your browser does not support the video tag.</video><br><p class="demo-text">Demo Introduction: "The video contains three parts; The first part presents<br>                        the localization accuracy and global consistency by comparing with the ground truth provided in<br>                        the indoor environment. Three trajectories of model-only, model+EKF and ground truth are shown<br>                        in different colors. The 3D model used for localization is shown as dense point cloud. The second<br>                        part shows the real-time localization results in outdoor case.<br>                        <br/><br>                        Four trajectories of model-only, model+EKF, VINS and GPS are represented in different colors. The<br>                        3D model used for localization is shown as sparse point cloud for display efficiencyconsideration.<br>                        The special outdoor case with unstable GPS is also included. Three image views including the raw<br>                        fisheye image, the electronically stabilized image, and the rendered image are shown simultaneously.<br>                        Besides comparing different trajectories, it is intuitive to tell the localization performance by<br>                        visually comparing the similarity between the stabilized image and the rendered image. The third<br>                        part of the video shows the closed-loop control by a trajectory tracking experiment using the<br>                        proposed method for state feedback."<br>                        </p><br></div><br></div><br></div><br></div><br></section><br><!-- Footer --><br><footer><br></footer><br><!-- Scroll to Top Button (Only visible on small and extra-small screen sizes) --><br><div class="scroll-top page-scroll hidden-sm hidden-xs hidden-lg hidden-md"><br><a class="btn btn-primary" href="#page-top"><br><i class="fa fa-chevron-up"></i><br></a><br></div><br><!-- Portfolio Modals --><br><div aria-hidden="true" class="portfolio-modal modal fade" id="portfolioModal1" role="dialog" tabindex="-1"><br><div class="modal-content"><br><div class="close-modal" data-dismiss="modal"><br><div class="lr"><br><div class="rl"><br></div><br></div><br></div><br><div class="container"><br><div class="row"><br><div class="col-lg-8 col-lg-offset-2"><br><div class="modal-body"><br><h2>Natural Language Processing</h2><br><hr class="star-primary"/><br><div class="r-item"><br><h3>Prof. Qiang Yang</h3><br><img alt="" src="img/ppl/QiangYang.jpg"/><br><span><br><h4>Machine Reading System</h4><br>                                        Project Abstract: Machine Reading aims to develop computer algorithms<br>                                        to understand text documents and to provide document-related service.<br>                                        One of its applications is automatic summarization, whose goal is to<br>                                        produce a summary of a given article (such as news, webpages or social<br>                                        network posts). The summary generated would contain the key information<br>                                        of the article, but is much shorter.<br>                                        <br/><br><br/><br><h4>Information Extraction System</h4><br>                                        Project Abstract: The function of the system could be separated into<br>                                        two parts: first, extracting the relation between two entities, regarded<br>                                        as answers, accurately; second, providing relevant evidences for the answers.<br>                                    </span><br></div><br><br/><br><br/><br><div class="r-item"><br><h3>Prof. Lei Chen</h3><br><img alt="" src="img/ppl/LeiChen.jpg"/><br><span><br><h4>Crowdsourcing Platform in WeChat</h4><br>                                        Project Abstract: Last year, we constructed a general crowdsourcing platform<br>                                        based on WeChat Service Account. Data labelling tasks can be posted on this<br>                                        platform for WeChat users to contribute their answers. Common tasks types (<br>                                        multiple choice, text input etc.) and some state-of-the-art crowdsourcing<br>                                        answer aggregation methods are supported. We also conducted some internal<br>                                        experiments with data from WeChat on it.<br>                                        <br/><br>                                        Based on the work last year, we will conduct more experiments of labelling<br>                                        task with monetary rewards. Different pricing algorithms will be used to<br>                                        reduce the task budget. Our ultimate goal is to make this platform as competitive<br>                                        as current 3rd party crowdsourced labelling platforms for future data labelling tasks.<br>                                    </span><br></div><br><br/><br/><br><button class="btn btn-default" data-dismiss="modal" type="button"><i class="fa fa-times"></i> Close</button><br></div><br></div><br></div><br></div><br></div><br></div><br><div aria-hidden="true" class="portfolio-modal modal fade" id="portfolioModal2" role="dialog" tabindex="-1"><br><div class="modal-content"><br><div class="close-modal" data-dismiss="modal"><br><div class="lr"><br><div class="rl"><br></div><br></div><br></div><br><div class="container"><br><div class="row"><br><div class="col-lg-8 col-lg-offset-2"><br><div class="modal-body"><br><h2>Information Propagation and Visualization</h2><br><hr class="star-primary"/><br><div class="r-item"><br><h3>Prof. Huamin Qu</h3><br><img alt="" src="img/ppl/HuaminQu.jpg"/><br><span><br><h4>Moments Articles Real Time Propagation and Visualization</h4><br>                                        Project Abstract: As a new type of social networking service, WeChat has<br>                                        already become ubiquitous in peoples daily mobile communication. Tracking<br>                                        in the pulse of WeChat information propagation is important. For corporations,<br>                                        it enables them to get feedback of user coverage, information diffusion<br>                                        patterns, and gain insight into how to improve and market better. Also, the<br>                                        abundance of information and opinions from diverse sources in WeChat platform<br>                                        help them tap into the wisdom of crowds, understand the retweeting/comment/like<br>                                        behaviors and aid in making more informed decisions. Meanwhile, WeChat generates<br>                                        enormous digital data archives recording various user activities, introducing a<br>                                        proliferation of opportunities to understand users communication behaviors.<br>                                        Analyzing these behaviors not only helps with finding the common communication<br>                                        patterns adopted by the public, but more importantly facilitates the detection<br>                                        of anomalous users who are potential threats to society.<br>                                    </span><br></div><br><br/><br><br/><br><div class="r-item"><br><h3>Prof. Yangqiu Song</h3><br><img alt="" src="img/ppl/YangqiuSong.jpg"/><br><span><br><h4>Emoji Propagation and its impact analysis in WeChat</h4><br>                                        Project Abstract: Wechat\'s emotional icon sets are being used more and more by<br>                                        their users. The recommendation and popularization of these icon sets is a great<br>                                        part of Wechat\'s revenue growth in 2017. The growth of the ecosphere of emotional<br>                                        icon sets will not only benefit Wechat\'s revenue, but will also create chances for<br>                                        content producers. Therefore, recognition, classification, advertising and<br>                                        recommendation of emotional icon sets are becoming important tasks. Compared with<br>                                        traditional texts and news, emotional icon sets have more unified structures;<br>                                        different from traditional emoji, emotional icon sets carry more vivid information<br>                                        through the large picture. Hence we should not merely use the previous analysis<br>                                        methods to analyse emotional icon sets. For example, when analyzing the propagation<br>                                        process, different group of people (young and old) have different tendency to use<br>                                        the icon sets. Also, when analyzing the propagation process, icons related to joy<br>                                        and anger will have different pattern of propagation, and sometimes even related<br>                                        to posted articles. For instance, when a article about massacre at Nanking is<br>                                        spreading, emotional icons about anger is spreaded at the same time. Therefore,<br>                                        analyzing the semantic of emotional icons is important and interesting. The project<br>                                        aims to introduce semantic information to the propagation of wechat icon sets, and<br>                                        establish unified representation with heterogeneous information networks(HIN), thus<br>                                        to analyse the features of propagation and find the crucial users. We hope that our<br>                                        research will help with Wechat\'s recommendation system of emotional icon sets.<br>                                    </span><br></div><br><br/><br><br/><br><button class="btn btn-default" data-dismiss="modal" type="button"><i class="fa fa-times"></i> Close</button><br></div><br></div><br></div><br></div><br></div><br></div><br><div aria-hidden="true" class="portfolio-modal modal fade" id="portfolioModal3" role="dialog" tabindex="-1"><br><div class="modal-content"><br><div class="close-modal" data-dismiss="modal"><br><div class="lr"><br><div class="rl"><br></div><br></div><br></div><br><div class="container"><br><div class="row"><br><div class="col-lg-8 col-lg-offset-2"><br><div class="modal-body"><br><h2>Large-scale Machine Learning</h2><br><hr class="star-primary"/><br><div class="r-item"><br><h3>Prof. Kai Chen</h3><br><img alt="" src="img/ppl/KaiChen.jpg"/><br><span><br><h4>RDMA Technology over Converged Ethernet in Large-scale Deep Learning System</h4><br>                                        Project Abstract: With the rapid growth of model complexity and data volume, deep<br>                                        learning systems require more and more Graphics Processing Units (GPU) to perform<br>                                        parallel training. Currently, deep learning systems with multiple servers and<br>                                        multiple GPUs are usually implemented in a single cluster, which typically employs<br>                                        Infiniband fabric to support Remote Direct Memory Access (RDMA), so as to achieve<br>                                        high throughput and low latency for inter-server transmission. It is expected that,<br>                                        with ever-larger models and data, deep learning systems must scale to multiple<br>                                        network clusters, which necessitates highly efficient inter-cluster networking stack<br>                                        with RDMA support. Since Infiniband is more suited for small-scale clusters of less<br>                                        than thousands of servers, we believe RDMA-over-Converged-Ethernet (RoCE) is a more<br>                                        appropriate networking technology choice for multi-cluster deep learning. Therefore,<br>                                        we collaborate with Tencent WeChat in the development of their in-house deep learning<br>                                        system, Amber, and incorporate RoCE as the networking technology. Achieving high<br>                                        throughput and low latency, Amber is also able to seamlessly scale to multiple clusters<br>                                        using our technology stack. In the process of development and usage, we encountered<br>                                        and dealt with many practical issues, such as PFC deadlock, PFC storm, RDMA live lock, etc.<br>                                    </span><br></div><br><br/><br><br/><br><button class="btn btn-default" data-dismiss="modal" type="button"><i class="fa fa-times"></i> Close</button><br></div><br></div><br></div><br></div><br></div><br></div><br><div aria-hidden="true" class="portfolio-modal modal fade" id="portfolioModal4" role="dialog" tabindex="-1"><br><div class="modal-content"><br><div class="close-modal" data-dismiss="modal"><br><div class="lr"><br><div class="rl"><br></div><br></div><br></div><br><div class="container"><br><div class="row"><br><div class="col-lg-8 col-lg-offset-2"><br><div class="modal-body"><br><h2>Video Analysis</h2><br><hr class="star-primary"/><br><div class="r-item"><br><h3>Prof. Qiang Yang</h3><br><img alt="" src="img/ppl/QiangYang.jpg"/><br><span><br><h4>Video Understanding in WeChat Moments</h4><br>                                        Project Abstract: Video captioning, describing video content with natural<br>                                        language, can be used for user behavior analysis, user modeling and video<br>                                        editing. Deep neural networks have achieved state-of-the-art performances<br>                                        on this task, but it is challenging to train deep neural networks when video<br>                                        clips lack textual descriptions, which is the case for wechat short videos.<br>                                        This project aims to learn a captioning model for wechat short videos by<br>                                        leveraging transfer learning.<br>                                    </span><br></div><br><br/><br><br/><br><br/><br><br/><br><button class="btn btn-default" data-dismiss="modal" type="button"><i class="fa fa-times"></i> Close</button><br></div><br></div><br></div><br></div><br></div><br></div><br><div aria-hidden="true" class="portfolio-modal modal fade" id="portfolioModal5" role="dialog" tabindex="-1"><br><div class="modal-content"><br><div class="close-modal" data-dismiss="modal"><br><div class="lr"><br><div class="rl"><br></div><br></div><br></div><br><div class="container"><br><div class="row"><br><div class="col-lg-8 col-lg-offset-2"><br><div class="modal-body"><br><h2>Robotics</h2><br><hr class="star-primary"/><br><div class="r-item"><br><h3>Prof. Qiang Yang</h3><br><img alt="" src="img/ppl/QiangYang.jpg"/><br><span><br><h4>Wifi Localization System</h4><br>                                        Project Abstract: Abstract- WIFI Indoor localization has attracted a lot<br>                                        of attention nowadays due to its deployment practicability and great<br>                                        importance in location based services(LBS). The existing WIFI indoor<br>                                        localization techniques often need a very time and labour consuming data<br>                                        collection procedure, which makes it impractical in large scale indoor<br>                                        positioning applications. In this project, we propose a novel technique<br>                                        for large scale indoor localization using WeChat crowdsourced WIFI big<br>                                        data. We present two key modules in this large scale WIFI localization<br>                                        system. First, a crowdsourcing fingerprint map algorithm is built by<br>                                        combining WIFI signal and floorplan map, as well as inertial sensor data<br>                                        signatures, which address the location labeling problem in large scale<br>                                        positioning. Second, a new localization method that combines cross-domain<br>                                        crowdsourcing data is integrated into indoor positioning system. The goal<br>                                        of this project is to build an accurate, but zero-effort indoor localization<br>                                        system that make indoor positioning possible in large scale applications.<br>                                    </span><br></div><br><br/><br><br/><br><div class="r-item"><br><h3>Prof. Xiaojuan Ma</h3><br><img alt="" src="img/ppl/XiaojuanMa.jpg"/><br><span><br><h4>Emotion Analysis based on Posture Sequence of Human Beings</h4><br>                                        Project Abstract: Human pose recognition is a crucial technique for robot<br>                                        to understand its human partners in Human-Robot Interaction (HRI). Recent<br>                                        human pose recognition studies often rely on video clips of very specific<br>                                        poses pre-segmented by human annotators, e.g., jumping in sport videos.<br>                                        However, such manual video segmentation may not be available for real-time<br>                                        pose recognition in HRI, imposing challenges to pose understanding from<br>                                        streaming pose data for an interactive robot. In this project, first, we will<br>                                        propose real-time algorithms to automatically segment human poses from streaming<br>                                        pose data of human participants in HRI, and then classify these pose segments<br>                                        into meaningful high-level categories, e.g., instructional poses (e.g., pointing),<br>                                        informational poses (e.g., gesturing the size of something), social poses (e.g.,<br>                                        hand shaking), and emotional poses (e.g., expression of anger etc., based on findings<br>                                        of existing interpersonal interaction research. Second, we will design associated<br>                                        strategies for the robot to parse and react appropriately to pose segments in each<br>                                        category. For example, for instructional poses, the robot will be programmed to<br>                                        recognize the commands and follow the commanders instructions. For informational<br>                                        poses, the robot will acknowledge the reception of information. For social poses,<br>                                        the robot will mimic the participants behaviours to increase the intimacy between<br>                                        the robot and its partners. For emotional poses, the robot will provide affective<br>                                        responses. Third, we will conduct a user study to demonstrate the performance of<br>                                        our algorithms and the effectiveness of the proposed strategies.<br>                                    </span><br></div><br><br/><br><br/><br><div class="r-item"><br><h3>Prof. Shaojie Shen</h3><br><img alt="" src="img/ppl/ShaojieShen.jpg"/><br><span><br><h4>Augmented Reality development based on monocular visual-inertial odometry</h4><br>                                        Project Abstract: With fast development of mobile computing devices and sensor<br>                                        technology, Augmented Reality (AR) has been a hot research topic in both academic<br>                                        and industrial fields. The technology is tightly relevant with Simultaneous<br>                                        Localization and Mapping (SLAM) in robotic areas, where the 6-Degree of Freedom<br>                                        (DoF) agent\'s pose and a description of the environment is simultaneously estimated<br>                                        in real-time. Another related area called Structure from Motion (SfM) has similar<br>                                        problem formulation but it cannot satisfy the real-time requirement of AR. Actually,<br>                                        object tracking and environment mapping is the key in today\'s AR research. Constructing<br>                                        a dense map by using the mature visual-inertial odometry technology, which can be<br>                                        regarded as the state estimation part of SLAM, is one feasible scheme at the current<br>                                        stage. The minimum sensing consists a monocular camera and an inertial measurement<br>                                        unit can also be satisfied with this scheme.<br>                                    </span><br></div><br><br/><br><br/><br><button class="btn btn-default" data-dismiss="modal" type="button"><i class="fa fa-times"></i> Close</button><br></div><br></div><br></div><br></div><br></div><br></div><br></div><br><div class="footer-section"><br><div class="footer-div"><br><div class="max1140-w"><br><div class="footer-main-row"><br><div class="footer-left"><a class="hkust-footer-logo w-inline-block" href="https://www.ust.hk" target="_blank"><img alt="HKUST" class="hkust-logo-w" src="images/hkust_logo_small.svg"/></a><br><div class="footer-left-inner"><a class="footer-links-left" href="mailto:bdi@ust.hk" target="_blank">Contact Us</a><a class="footer-links-left" href="https://www.ust.hk/privacy-policy/" target="_blank">Privacy</a><br><div class="footer-copyright">Copyright  The Hong Kong University of Science and Technology.<span class="arr">All rights reserved.</span></div><br></div><br></div><br><div class="footer-right-div"><br><div class="footer-follow">Follow HKUST on</div><br><div class="footer-socal-div"><a class="footer-social w-inline-block" href="http://facebook.com/HKUST" target="_blank"><img alt="Facebook" src="images/s-icon-fb.svg" width="32"/></a><a class="footer-social w-inline-block" href="https://www.instagram.com/hkust/" target="_blank"><img alt="Instagram" src="images/s-icon-ig.svg" width="32"/></a><a class="footer-social w-inline-block" href="https://www.linkedin.com/school/11966" target="_blank"><img alt="Linkedin" src="images/s-icon-in.svg" width="32"/></a><a class="footer-social w-inline-block" href="http://www.youtube.com/user/HKUST" target="_blank"><img alt="YouTube" src="images/s-icon-youtube.svg" width="32"/></a></div><br></div><br><div class="footer-mobile-only-nav"><a class="footer-links-mo" href="https://www.ust.hk/privacy-policy/" target="_blank">Contact Us</a><a class="footer-links-mo" href="https://www.ust.hk/privacy-policy/" target="_blank">Privacy</a></div><br></div><br></div><br></div><br></div><br><script crossorigin="anonymous" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" src="https://code.jquery.com/jquery-3.3.1.min.js" type="text/javascript"></script><br><script src="js/whatlab.js" type="text/javascript"></script><br><!-- [if lte IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif] --><br><!--  jQuery  --><br><script src="vendor/jquery/jquery.min.js"></script><br><!--  Bootstrap Core JavaScript  --><br><script src="vendor/bootstrap/js/bootstrap.min.js"></script><br><!--  Plugin JavaScript  --><br><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script><br><!--  Theme JavaScript  --><br><script src="js/freelancer.js"></script><br><!--  Custom JavaScript  --><br><script src="js/main.js"></script><br></body><br></html><br>'